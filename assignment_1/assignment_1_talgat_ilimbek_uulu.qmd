---
title: "Assignment 1: Predicting Hourly wage of Financial Managers using Linear Regressions "
author: "Talgat Ilimbek uulu"
format: pdf
editor: visual
bibliography: references.bib
---

```{r include=FALSE}
# Loading packages
library(tidyverse)
library(readr)
library(modelsummary)
library(fixest)
library(lmtest)
library(sandwich)
library(stargazer)
library(caret)
library(grid)
library(RColorBrewer)
library(ggthemes)
library(gridExtra)
library(lmtest)
library(sandwich)
```

```{r include=FALSE}
# Loading data and filtering by occupation code
sample <- read_csv("raw_data/morg-2014-emp.csv") %>% 
  filter(occ2012 == "120")
```

```{r include=FALSE}
## Preparing data for the analysis

# creating dependent variable hourly_wage and log hourly_wage
sample$hourly_wage <- sample$earnwke/sample$uhours
sample$log_wage <- log(sample$hourly_wage)

# creating necessary dummy variables
sample$male <- ifelse(sample$sex == 1, 1, 0)
sample$union <- ifelse(sample$unionmme == "Yes", 1, 0)
sample$married <- ifelse(sample$marital %in% c(1,2,3), 1, 0)
sample$widowed <- ifelse(sample$marital == 4, 1, 0)
sample$divorced <- ifelse(sample$marital == 5, 1, 0)
sample$separated <- ifelse(sample$marital == 6, 1, 0)
sample$never_married <- ifelse(sample$marital == 7, 1, 0)
sample$native_born_us <- ifelse(sample$prcitshp == "Native, Born In US", 1, 0)
sample$white <- ifelse(sample$race == 1, 1, 0)
sample$black <- ifelse(sample$race == 2, 1, 0)
sample$asian <- ifelse(sample$race == 4, 1, 0)
sample$hard_workers <- ifelse(sample$uhours > 50, 1, 0)
sample$no_child <- ifelse(sample$ownchild == 0, 1, 0)

# transforming education variable into years of schooling
sample$educ <- ifelse(sample$grade92 == 34, 8, sample$grade92)
sample$educ <- ifelse(sample$grade92 == 35, 9, sample$educ)
sample$educ <- ifelse(sample$grade92 == 36, 10, sample$educ)
sample$educ <- ifelse(sample$grade92 == 37, 11, sample$educ)
sample$educ <- ifelse(sample$grade92 == 39, 12, sample$educ)
sample$educ <- ifelse(sample$grade92 == 40, 13, sample$educ)
sample$educ <- ifelse(sample$grade92 == 41, 14, sample$educ)
sample$educ <- ifelse(sample$grade92 == 42, 14, sample$educ)
sample$educ <- ifelse(sample$grade92 == 43, 16, sample$educ)
sample$educ <- ifelse(sample$grade92 == 44, 18, sample$educ)
sample$educ <- ifelse(sample$grade92 == 45, 22, sample$educ)
sample$educ <- ifelse(sample$grade92 == 46, 22, sample$educ)

# creating binning variable for grade92
sample$educ_bin <- ifelse(sample$grade92 <= 38, "no_high_school_degree", sample$grade92)
sample$educ_bin <- ifelse(sample$grade92 == 39, "high_school_degree", sample$educ_bin)
sample$educ_bin <- ifelse(sample$grade92 == 40, "some_college", sample$educ_bin)
sample$educ_bin <- ifelse(sample$grade92 >= 41, "college_degree", sample$educ_bin)

# creating experience variable
sample$exper <- ifelse(sample$educ_bin == "no_high_school_degree", sample$age - 17, sample$educ_bin)
sample$exper <- ifelse(sample$educ_bin == "high_school_degree", sample$age - 19, sample$exper)
sample$exper <- ifelse(sample$educ_bin == "some_college", sample$age - 21, sample$exper)
sample$exper <- ifelse(sample$educ_bin == "college_degree", sample$age - 23, sample$exper)
sample$exper <- ifelse(sample$exper < 0, 0, sample$exper) # if experience is negative, then 0
sample$exper <- as.numeric(sample$exper)
sample$expersq <- sample$exper^2

# creating a regional variables
sample$top_states <- ifelse(sample$stfips %in% c("NY","NJ","DE","CO","CA","MA","RI"), 1, 0)
sample$low_states <- ifelse(sample$stfips %in% c("MS","AR","WV","NM","ID"), 1, 0)

# creating an industry dummy
sample$top_industry <- ifelse(sample$ind02 == "Securities, commodities, funds, trusts, and other financial investments (523, 525)"|sample$ind02 == "Computer systems design and related services (5415)",1,0)

# 2 observations with negative log_wage are deleted (significant outliers and highly likely to be errors)
sample <- sample %>% 
  filter(log_wage > 0)

```

# Motivation

The goal of this assignment is to predict hourly wage of financial managers in US market. The starting point of our study will be a human capital theory. In labor economics, human capital is generally referred to the stock of skills and characteristics possessed by workers that increase their productivity. Human capital research has traditionally used educational attainment as a way to measure human capital. The work by @mincer74 proposes a well-known single-equation model that establishes a link between wages, schooling, and experience in the labor market.

$$lnw = f(s,x) = \beta_0 + ps + \beta_1x + \beta_2x^2$$

where $w$ is wage, $s$ is a years of schooling and $x$ is a potential experience in a labor market.

The notable advantage of the Mincer equation is that it has a robust theoretical foundations. Mincer equation will serve as a basis upon which I plan to develop and refine additional three models to explore and analyze the data.

# Data

In the sample, we have 1399 individuals in the "financial managers" occupation, of whom 71% hold a college degree. The dependent variable is the natural logarithm of earnings per hour, calculated as the ratio of weekly earnings to usual hours worked. Regarding the education variable, the data employs a credential-oriented measure. To align with the model, I transformed the variable into years of schooling, using standard duration for the obtained degrees. For example, a PhD degree is considered equivalent to 22 years of schooling, MA degree to 18 years of schooling, etc. Years of potential experience are computed as the difference between current age and the age of potential entry into the labor market. This calculation assumes that individuals without a high school degree enter the labor force at age 17, those with a high school degree at 19, those with some college at 21, and those with a college degree at 23. see @borjas2003

```{r include=FALSE}
# graph1

p1 <- ggplot(data = sample, aes(x = educ, y = log_wage)) +
  geom_jitter(width = 0.08,color = "azure4", size = 1.5,  shape = 16, alpha = 0.8, show.legend=FALSE, na.rm=TRUE) +
  geom_smooth(method="loess", color="blueviolet", se=F, size=0.8, na.rm=T)+
  labs(x = "Education (years)", y = "ln(Hourly wage, US dollars)") + theme_linedraw() 

p2 <- ggplot(data = sample, aes(x = exper, y = log_wage)) +
  geom_jitter(width = 0.08,color = "azure4", size = 1.5,  shape = 16, alpha = 0.5, show.legend=FALSE, na.rm=TRUE) +
  geom_smooth(method="loess", color="blueviolet", se=F, size=0.8, na.rm=T)+
  labs(x = "Experience (years)", y = "ln(Hourly wage, US dollars)") + theme_linedraw() 

graph1 <- grid.arrange(p1, p2, ncol=2)
ggsave("graphs/graph1.png",plot = graph1, width = 10, height = 5, units = "in", dpi = 300)
```

![Log hourly wage, education and experience](graphs/graph1.png){fig-align="center" width="611"}

To check whether functional form suggested by theory is consistent with the data, I estimate a lowess regression and plot together with the scatterplot (see Figure 1). According to the pattern uncovered by lowess, average earnings increase with each additional year of schooling and experience shows the concave pattern. The standard regression table of the model is presented in Table 1.

```{r include=FALSE}
# Table 1 regression of log_wage on educ, exper and summarize results
regression_1 <- lm(log_wage ~ educ + exper + expersq, data = sample)
summary(regression_1)  
coeftest(regression_1, vcov = sandwich)
```

```{=tex}
\begin{table}[ht]
\caption{Mincer equation OLS estimates} 
\centering 
\begin{tabular}{l r } 
\hline\hline 
 & Ln wage \\ [0.5ex] 
\hline 
Education & .096*** (.006) \vspace{1mm}\\
Experience & .041*** (.004) \vspace{1.5mm}\\
Experience squared & -.0006*** (.0001) \vspace{1.5mm}\\
Constant & 1.386*** (.10) \vspace{1.5mm}\\
Number of observations & 1399 \\
Adjusted R-squared & 0.25 \\
\hline
\multicolumn{2}{l}{\textsuperscript{***}$p<0.01$, 
  \textsuperscript{**}$p<0.05$, 
  \textsuperscript{*}$p<0.1$}
  Robust standard errors in parenthesis
\end{tabular}
\end{table}
```
The OLS regression of log hourly wage to year of schooling and experience implies that 1 year of additional schooling is associated with 9.6% increase in monthly wage for financial managers. Both figures and regression give evidence supporting the argument that education and experience may play an important role in the wage determination. Note, however, all correlations should be interpreted with caution and shouldn't be given causal interpretation. The positive correlation between wage and education can reflect unobserved family and individual heterogeneity.

# Four regression models

I have specified four linear regression models for predicting hourly wage of financial managers.\
The Model 1 is our baseline Mincer equation capturing established factors influencing wages. Building upon this foundation, I introduce three subsequent models, each incorporating additional variables used in the literature to enhance the predictive capacity (see Appendix for additional table and graphs)

```{=tex}
\begin{table}[ht]
\caption{Wage baseline regression} 
\centering 
\begin{tabular}{l l } 
\hline\hline Model No. & Used variables \\ 
[0.5ex] 
\hline 
Model 1 & education, experience, experience squared \vspace{1mm}\\
Model 2 & Model 1 + male, married, black,no children, native born  \vspace{1.5mm}\\
Model 3 & Model 2 + dummies for certain states and industries  \vspace{1.5mm}\\
Model 4 & Model 3 + interaction terms \vspace{1.5mm}\\
\hline
\end{tabular}
\end{table}
```
Model 2 augments the baseline Mincer equation by introducing demographic features such as gender, race, and marital status. These variables are crucial predictors, capturing disparities like well documented gender-based wage gaps and the influence of race and marital status on career opportunities. Model 3 enhances the Mincer equation by including data on the highest and lowest paying states and industries. This addition is crucial as it captures geographic and industry-specific factors influencing wage variations. Model 4 adds interaction terms to the previous model. The results are presented at Table 3.

```{r include=FALSE}
# Linear regressions in logs (adopted from the case study codes)
# Model 1: Linear regression on educ
model1log <- as.formula(log_wage ~ educ + exper + expersq)
# Models 2-4: 
model2log <- as.formula(log_wage ~ educ + exper + expersq + male + married + black + no_child + native_born_us)
model3log <- as.formula(log_wage ~ educ + exper + expersq + male + married + black + no_child + native_born_us + top_states+low_states+hard_workers+top_industry)
model4log <- as.formula(log_wage ~ educ + exper + expersq + male + married + black + no_child +native_born_us + top_states+low_states+hard_workers+top_industry + black*male + married*male + top_states*top_industry,low_states*top_industry+male*no_child + educ*male + educ*black)
reg1log <- lm(model1log, data=sample)
reg2log <- lm(model2log, data=sample)
reg3log <- lm(model3log, data=sample)
reg4log <- lm(model4log, data=sample)
# evaluation of the models
models <- c("reg1log", "reg2log","reg3log", "reg4log")
AIC <- c()
BIC <- c()
RMSE <- c()
RSquared <- c()
regr <- c()
k <- c()

for ( i in 1:length(models)){
  AIC[i] <- AIC(get(models[i]))
  BIC[i] <- BIC(get(models[i]))
  RMSE[i] <- RMSE(predict(get(models[i])), get(models[i])$model$log_wage)
  RSquared[i] <-summary(get(models[i]))$r.squared
  regr[[i]] <- coeftest(get(models[i]), vcov = sandwich)
  k[i] <- get(models[i])$rank -1
}

############################################################
# Linear regression evaluation
# All models
eval <- data.frame(models, k, RSquared, RMSE, BIC)
eval <- eval %>%
  mutate(models = paste0("(",gsub("reg","",models),")")) %>%
  rename(Model = models, "R-squared" = RSquared, "Training RMSE" = RMSE, "N predictors" = k)
stargazer(eval, summary = F, digits=2, float = F, no.space = T)
```

```{=tex}
\begin{table}[ht]
\caption{Wage models and measures of fit} 
\centering 
\begin{tabular}{@{\extracolsep{5pt}} cccccc} 
\\[-1.8ex]
\hline 
\hline \\[-1.8ex] 
 & Model & N predictors & R-squared & Training RMSE & BIC \\ 
\hline \\[-1.8ex] 
  & Model 1 & $3$ & $0.25$ & $0.456$ & $1,810.29$ \\ 
  & Model 2 & $8$ & $0.30$ & $0.442$ & $1,760.30$ \\ 
  & Model 3 & $12$ & $0.31$ & $0.437$ & $1,757.15$ \\ 
  & Model 4 & $15$ & $0.32$ & $0.436$ & $1,770.64$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}
```
Referring to Table 3, both Model 3 - 4 have comparable RMSE, however the Model 3 has the lowest BIC (1757.15) which suggests that it has better prediction properties compared to other models. Notice that once we increase the complexity of the model by increasing the number of predictors, R-squared increases as well, which is expected. Potentially, we could keep adding more predictors to the model and increase the R-squared, however, this would not necessarily improve the prediction properties of the model and could lead to overfitting. It is suggested to use BIC as a criterion for model selection, which penalizes the number of parameters in the model.

```{r include=FALSE}
# Cross-validation (adopted from the case study codes)
# set number of folds (4 because of small sample)
k <- 4
# need to set the same seed again and again
set.seed(081123)
cv1log <- train(model1log, sample, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(081123)
cv2log <- train(model2log, sample, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(081123)
cv3log <- train(model3log, sample, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")
set.seed(081123)
cv4log <- train(model4log, sample, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")

# calculate average rmse
cv <- c("cv1log", "cv2log", "cv3log", "cv4log")
rmse_cv <- c()

for(i in 1:length(cv)){
  rmse_cv[i] <- sqrt((get(cv[i])$resample[[1]][1]^2 +
                        get(cv[i])$resample[[1]][2]^2 +
                        get(cv[i])$resample[[1]][3]^2 +
                        get(cv[i])$resample[[1]][4]^2)/4)
}


# summarize results
cv_matlog <- data.frame(rbind(cv1log$resample[4], "Average"),
                        rbind(cv1log$resample[1], rmse_cv[1]),
                        rbind(cv2log$resample[1], rmse_cv[2]),
                        rbind(cv3log$resample[1], rmse_cv[3]),
                        rbind(cv4log$resample[1], rmse_cv[4])
)

colnames(cv_matlog)<-c("Resample","Model1log", "Model2log", "Model3log", "Model4log")
cv_matlog
stargazer(cv_matlog, summary = F, digits=3, float=F, sep="")

```

```{=tex}
\begin{table}[ht]
\caption{Wage models estimated and evaluated using 4-fold cross-validation and RMSE} 
\centering 
\begin{tabular}{@{\extracolsep{5pt}} ccccc} \\[-1.8ex]
\hline 
\hline \\[-1.8ex] 
 Fold No. & Model 1 & Model 2 & Model 3 & Model 4 \\ 
\hline
Fold1 & $0.434$ & $0.413$ & $0.412$ & $0.411$ \\ 
Fold2 & $0.459$ & $0.450$ & $0.446$ & $0.446$ \\ 
Fold3 & $0.473$ & $0.460$ & $0.460$ & $0.458$ \\ 
Fold4 & $0.465$ & $0.459$ & $0.450$ & $0.452$ \\ 
Average & $0.458$ & $0.446$ & $0.442$ & $0.442$ \\ 
\hline \\[-1.8ex] 
\end{tabular}
\end{table}
```
Referring to Table 4, both Model 3 and Model 4 have the lowest average RMSE. This finding aligns with previous studies where both BIC and cross-validation often converge on the same optimal model. Given that Model 3 not only has the lowest BIC and average RMSE but is also less complex compared to Model 4, I choose it as the best model. The table below shows the point prediction of our model

```{r include=FALSE}
# Prediction
data <- sample %>% dplyr::select(educ,exper,expersq,male,married,black, no_child,native_born_us,
                                 top_states,low_states,hard_workers,top_industry,log_wage)
# Add new observation
new <- list(educ = 18, exper = 11, expersq = 121,male = 1,married = 0,black = 0, no_child = 1,native_born_us = 1,
            top_states = 1,low_states = 0,hard_workers = 1,top_industry = 1, log_wage=NA)
reg3 <- lm(log_wage ~ educ + exper + expersq + male + married + black + no_child + native_born_us + 
             top_states+low_states+hard_workers+top_industry, data=data)
summary(reg3)
# prediction
data$lnp2 <- predict(reg3, data)
rmse3 <- RMSE(data$lnp2,data$log_wage)

# prediction for new observation
predln_new <- predict(reg3, newdata = new,se.fit = TRUE, interval = "prediction")
predln_new80 <- predict(reg3, newdata = new,se.fit = TRUE, interval = "prediction", level=0.90)
predln_new80
lnp2_new <- predln_new$fit[[1]]

# predictions in levels
data$lnplev <- exp(data$lnp2)*exp((rmse3^2)/2)
lnp2_new_lev <- exp(lnp2_new)*exp((rmse3^2)/2)

# prediction intervals (log and level)
lnp2_PIlow <- predln_new$fit[2]
lnp2_PIhigh <- predln_new$fit[3]
lnplev_PIlow <- exp(lnp2_PIlow)*exp(rmse3^2/2)
lnplev_PIhigh <- exp(lnp2_PIhigh)*exp(rmse3^2/2)

#80%
lnp2_PIlow80 <- predln_new80$fit[2]
lnp2_PIhigh80 <- predln_new80$fit[3]
lnplev_PIlow80 <- exp(lnp2_PIlow80)*exp(rmse3^2/2)
lnplev_PIhigh80 <- exp(lnp2_PIhigh80)*exp(rmse3^2/2)

# summary of predictions and PI
sum <- matrix( c( lnp2_new, lnp2_PIlow ,lnp2_PIhigh,
                  lnp2_new_lev, lnplev_PIlow, lnplev_PIhigh) , nrow = 3 ,ncol = 2)

colnames(sum) <- c('Model in logs', 'Recalculated to level')
rownames(sum) <- c('Predicted', 'PI_low', 'PI_high')
sum

sum <- matrix( c( lnp2_new, lnp2_PIlow80 ,lnp2_PIhigh80,
                  lnp2_new_lev, lnplev_PIlow80, lnplev_PIhigh80) , nrow = 3 ,ncol = 2)

colnames(sum) <- c('Model in logs', 'Recalculated to level')
rownames(sum) <- c('Predicted', 'PI_low 80%', 'PI_high 80%')

stargazer(sum, type = "latex", float=F, digits=2)


```

```{=tex}
\begin{table}[ht]
\caption{Point and interval prediction for specific individual using Model 3} 
\centering 
\begin{tabular}{@{\extracolsep{5pt}} ccc} \\
[-1.8ex]
\hline 
\hline \\[-1.8ex]  & Model in logs & Recalculated to level \\ \hline \\[-1.8ex] 
Predicted & $3.67$ & $43.36$ \\ 
PI\_low 80\% & $2.94$ & $20.88$ \\ 
PI\_high 80\% & $4.40$ & $90.04$ \\ 
\hline \\[-1.8ex] 
\end{tabular}
\end{table}
```
For this specific individual, the hourly wage is estimated to range from 21 to 90 dollars with an 80% probability. As anticipated, this is a broad prediction interval. Nevertheless, the lower limit of 21 dollars can serve as a benchmark for this specific individual planning to work as financial manager, enabling him to make informed adjustments to his budgetary considerations.

# References

::: {#refs}
:::

# Appendix

```{=tex}
\begin{table}[H]
\caption{Dependent variable Ln(Hourly wage), OLS Estimates.} 
\vspace*{3mm}
\centering 
\begin{tabular}{p{7cm} l l l l  }
\hline\hline 
& (1) & (2) & (3) & (4) \\ [0.5ex] 
\hline
Education  &0.097***&0.08***&0.08*** &0.08***  \\
           &(0.006) &(0.006)  & (0.006) & (0.006) \\            
Experience  &0.04***&0.037*** &0.036*** &0.036*** \\
           &(0.004) &(0.005) & (0.005)&(0.005) \\            
Experience squared &-0.0006***&-0.0005*** & -0.0005***& -0.0005***\\
                &(0.000)&(0.000) &(0.000) &(0.000) \\
Constant &1.39***&1.52*** & 1.52*** &1.53***  \\
                &(0.09)&(0.10) &(0.10) &(0.11)  \\            


Dummies  \\
\hspace{5mm} Demographic & No   &Yes   & Yes  & Yes  \\
\hspace{5mm} Regional  & No  &No  & Yes   & Yes   \\
\hspace{5mm} Interaction terms  & No  &No  & No   & Yes   \\
\hspace{5mm} Industry &No  &No  &No & Yes   \vspace{1.5mm} \\
\hline
Number of observations & 1399 & 1399 & 1399 & 1399 \\
Adj. R-squared & 0.25 & 0.29 & 0.30 & 0.31 \\
\hline
\multicolumn{3}{l}{\textsuperscript{***}$p<0.01$, 
  \textsuperscript{**}$p<0.05$, 
  \textsuperscript{*}$p<0.1$}
 \vspace{0.2cm}
Robust standard errors in parenthesis.
\end{tabular}
\label{table:regression5} 
\end{table}
```
```{r include=FALSE}
# graph2 boxplot
p3 <- ggplot(data = sample, aes(x = as.factor(male), y = hourly_wage)) +
  geom_boxplot(color = "blueviolet",  alpha = 0.8, show.legend=FALSE, na.rm=TRUE) +
  labs( y = "ln(Hourly wage, US dollars)", x = " ") + 
  scale_x_discrete(labels = c("Female", "Male"))+
  theme_linedraw()

p4 <- ggplot(data = sample, aes(x = as.factor(black), y = hourly_wage)) +
  geom_boxplot(color = "blueviolet",  alpha = 0.8, show.legend=FALSE, na.rm=TRUE) +
  labs( y = "ln(Hourly wage, US dollars)", x = " ") + 
  scale_x_discrete(labels = c("Others", "Black"))+
  theme_linedraw()

p5 <- ggplot(data = sample, aes(x = as.factor(married), y = hourly_wage)) +
  geom_boxplot(color = "blueviolet",  alpha = 0.8, show.legend=FALSE, na.rm=TRUE) +
  labs( y = "ln(Hourly wage, US dollars)", x = " ") + 
  scale_x_discrete(labels = c("Others", "Married"))+
  theme_linedraw()

graph2 <- grid.arrange(p3, p4, p5, ncol=3)
ggsave("graphs/graph2.png",plot = graph2, width = 10, height = 5, units = "in", dpi = 300)

```

![Hourly wage distribution by important demographic features](graphs/graph2.png){fig-align="center" width="523"}

```{r include=FALSE}

# graph3 boxplot
p6 <- ggplot(data = sample, aes(x = as.factor(top_states), y = hourly_wage)) +
  geom_boxplot(color = "blueviolet",  alpha = 0.8, show.legend=FALSE, na.rm=TRUE) +
  labs( y = "ln(Hourly wage, US dollars)", x = " ") + 
  scale_x_discrete(labels = c("Others", "Top Paying States"))+
  theme_linedraw()

p7 <- ggplot(data = sample, aes(x = as.factor(low_states), y = hourly_wage)) +
  geom_boxplot(color = "blueviolet",  alpha = 0.8, show.legend=FALSE, na.rm=TRUE) +
  labs( y = "ln(Hourly wage, US dollars)", x = " ") + 
  scale_x_discrete(labels = c("Others", "Low Paying States"))+
  theme_linedraw()

p8 <- ggplot(data = sample, aes(x = as.factor(top_industry), y = hourly_wage)) +
  geom_boxplot(color = "blueviolet",  alpha = 0.8, show.legend=FALSE, na.rm=TRUE) +
  labs( y = "ln(Hourly wage, US dollars)", x = " ") + 
  scale_x_discrete(labels = c("Others", "Top Paying Industries"))+
  theme_linedraw()

graph3 <- grid.arrange(p6, p7, p8, ncol=3)
ggsave("graphs/graph3.png",plot = graph3, width = 10, height = 5, units = "in", dpi = 300)

```

![Hourly wage distribution by important regional and industry features](graphs/graph3.png){fig-align="center" width="523"}
