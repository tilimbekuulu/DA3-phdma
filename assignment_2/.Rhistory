as.matrix() %>%
as.data.frame() %>%
rownames_to_column(var = "variable") %>%
rename(coefficient = `s1`)
print(lasso_coeffs)
lasso_coeffs_nz<-lasso_coeffs %>%
filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))
# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
filter(lambda == lasso_model$bestTune$lambda) %>%
dplyr::select(RMSE)
print(lasso_cv_rmse[1, 1])
# Diagnostics #
###################################################
model3_level <- model_results_cv[["modellev3"]][["model_work_data"]]
model7_level <- model_results_cv[["modellev7"]][["model_work_data"]]
# look at holdout RMSE
model7_level_work_rmse <- mse_lev(predict(model7_level, newdata = data_work), data_work[,"price"] %>% pull)**(1/2)
model7_level_holdout_rmse <- mse_lev(predict(model7_level, newdata = data_holdout), data_holdout[,"price"] %>% pull)**(1/2)
model7_level_holdout_rmse
# Target variable
Ylev <- data_holdout[["price"]]
meanY <-mean(Ylev)
sdY <- sd(Ylev)
meanY_m2SE <- meanY -1.96 * sdY
meanY_p2SE <- meanY + 1.96 * sdY
Y5p <- quantile(Ylev, 0.05, na.rm=TRUE)
Y95p <- quantile(Ylev, 0.95, na.rm=TRUE)
# Predicted values
predictionlev_holdout_pred <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="predict")) %>%
rename(pred_lwr = lwr, pred_upr = upr)
predictionlev_holdout_conf <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="confidence")) %>%
rename(conf_lwr = lwr, conf_upr = upr)
predictionlev_holdout <- cbind(data_holdout[,c("price","n_accommodates")],
predictionlev_holdout_pred,
predictionlev_holdout_conf[,c("conf_lwr","conf_upr")])
# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_holdout[,"fit"] )
# Check the differences
d$elev <- d$ylev - d$predlev
# Plot predicted vs price
level_vs_pred <- ggplot(data = d) +
geom_point(aes(y=ylev, x=predlev), color = color[1], size = 1,
shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
#geom_smooth(aes(y=ylev, x=predlev), method="lm", color=color[2], se=F, size=0.8, na.rm=T)+
geom_segment(aes(x = 0, y = 0, xend = 650, yend =650), size=0.5, color=color[2], linetype=2) +
coord_cartesian(xlim = c(0, 650), ylim = c(0, 650)) +
scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 650), breaks=seq(0, 650, by=50)) +
scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 650), breaks=seq(0, 650, by=50)) +
labs(y = "Price (US dollars)", x = "Predicted price  (US dollars)") +
theme_bg()
level_vs_pred
# Redo predicted values at 80% PI
predictionlev_holdout_pred <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="predict", level=0.8)) %>%
rename(pred_lwr = lwr, pred_upr = upr)
predictionlev_holdout_conf <- as.data.frame(predict(model7_level, newdata = data_holdout, interval="confidence", level=0.8)) %>%
rename(conf_lwr = lwr, conf_upr = upr)
predictionlev_holdout <- cbind(data_holdout[,c("price","n_accommodates")],
predictionlev_holdout_pred,
predictionlev_holdout_conf[,c("conf_lwr","conf_upr")])
summary(predictionlev_holdout_pred)
predictionlev_holdout_summary <-
predictionlev_holdout %>%
group_by(n_accommodates) %>%
dplyr::summarise(fit = mean(fit, na.rm=TRUE), pred_lwr = mean(pred_lwr, na.rm=TRUE), pred_upr = mean(pred_upr, na.rm=TRUE),
conf_lwr = mean(conf_lwr, na.rm=TRUE), conf_upr = mean(conf_upr, na.rm=TRUE))
kable(x = predictionlev_holdout_summary, format = "latex", booktabs=TRUE,  digits = 3, row.names = FALSE,
linesep = "", col.names = c("Accomodates","Prediction","Pred. interval lower",
"Pred. interval upper","Conf.interval lower","Conf.interval upper"))
F14_CI_n_accomodate <- ggplot(predictionlev_holdout_summary, aes(x=factor(n_accommodates))) +
geom_bar(aes(y = fit ), stat="identity",  fill = color[1], alpha=0.7 ) +
geom_errorbar(aes(ymin=pred_lwr, ymax=pred_upr, color = "Pred. interval"),width=.2) +
#geom_errorbar(aes(ymin=conf_lwr, ymax=conf_upr, color = "Conf. interval"),width=.2) +
scale_y_continuous(name = "Predicted price (US dollars)") +
scale_x_discrete(name = "Accomodates (Persons)") +
scale_color_manual(values=c(color[2], color[2])) +
theme_bg() +
theme(legend.title= element_blank(),legend.position="none")
F14_CI_n_accomodate
model_result_plot_levels <- ggplot(data = t1_levels,
aes(x = factor(nvars2), y = value, color=factor(var), group = var)) +
geom_line(size=1,show.legend=FALSE, na.rm = TRUE) +
scale_color_manual(name="",
values=c(color[2],color[1])) +
scale_x_discrete( name = "Number of coefficients", expand=c(0.01, 0.01)) +
geom_dl(aes(label = var),  method = list("last.points", dl.trans(x=x-1), cex=0.4)) +
#scale_colour_discrete(guide = 'none') +
theme_bg()
model_result_plot_levels
# save model_result plot in graphs folder
ggsave("graphs/model_results.png", model_result_plot_levels, width = 10, height = 6, units = "in", dpi = 300)
level_vs_pred <- ggplot(data = d) +
geom_point(aes(y=ylev, x=predlev), color = color[1], size = 1,
shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
#geom_smooth(aes(y=ylev, x=predlev), method="lm", color=color[2], se=F, size=0.8, na.rm=T)+
geom_segment(aes(x = 0, y = 0, xend = 650, yend =650), size=0.5, color=color[2], linetype=2) +
coord_cartesian(xlim = c(0, 650), ylim = c(0, 650)) +
scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 650), breaks=seq(0, 650, by=50)) +
scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 650), breaks=seq(0, 650, by=50)) +
labs(y = "Price (US dollars)", x = "Predicted price  (US dollars)") +
theme_bg()
level_vs_pred
# save plot in graphs folder
ggsave("graphs/level_vs_pred.png", plot = level_vs_pred, width = 6, height = 6, units = "in", dpi = 300)
F14_CI_n_accomodate <- ggplot(predictionlev_holdout_summary, aes(x=factor(n_accommodates))) +
geom_bar(aes(y = fit ), stat="identity",  fill = color[1], alpha=0.7 ) +
geom_errorbar(aes(ymin=pred_lwr, ymax=pred_upr, color = "Pred. interval"),width=.2) +
#geom_errorbar(aes(ymin=conf_lwr, ymax=conf_upr, color = "Conf. interval"),width=.2) +
scale_y_continuous(name = "Predicted price (US dollars)") +
scale_x_discrete(name = "Accomodates (Persons)") +
scale_color_manual(values=c(color[2], color[2])) +
theme_bg() +
theme(legend.title= element_blank(),legend.position="none")
F14_CI_n_accomodate
# save plot in graphs folder
ggsave("graphs/accomodates.png", plot = F14_CI_n_accomodate, width = 6, height = 6, units = "in", dpi = 300)
rm(list=ls())
library(rattle)
library(tidyverse)
library(caret)
library(ranger)
library(Hmisc)
library(knitr)
library(kableExtra)
library(xtable)
#location of folders
data_in  <- "raw_data/"
data_out <- "clean_data/"
# set data dir, load theme and functions
source("ch00-tech-prep/theme_bg.R")
source("ch00-tech-prep/da_helper_functions.R")
# Import the cleaned data
data <- read_csv(paste(data_out,"airbnb_sydney_cleaned.csv", sep = ""))
data2 <- read_csv(paste(data_out,"airbnb_sydney_cleaned_june_holdout.csv", sep = ""))
# CLEAR MEMORY
rm(list=ls())
library(rattle)
library(tidyverse)
library(caret)
library(ranger)
library(Hmisc)
library(knitr)
library(kableExtra)
library(xtable)
#location of folders
data_in  <- "raw_data/"
data_out <- "clean_data/"
# set data dir, load theme and functions
source("ch00-tech-prep/theme_bg.R")
source("ch00-tech-prep/da_helper_functions.R")
# Import the cleaned data
data <- read_csv(paste(data_out,"airbnb_sydney_cleaned.csv", sep = ""))
data2 <- read_csv(paste(data_out,"airbnb_sydney_cleaned_june_holdout.csv", sep = ""))
####################################################################################
# create train and holdout samples -------------------------------------------
# train is where we do it all, incl CV
set.seed(02122023)
train_indices <- as.integer(createDataPartition(data$price, p = 0.7, list = FALSE))
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]
dim(data_train)
dim(data_holdout)
# Define models:
# Basic Variables inc neighnourhood
basic_vars <- c("n_accommodates","n_beds","n_bedroom_count", "f_room_type",
"f_bathroom","f_minimum_nights","f_neighbourhood_cleansed")
# reviews
reviews <- reviews <- c("f_number_of_reviews","d_host_is_superhost",
"d_host_has_profile_pic","d_host_identity_verified","d_instant_bookable")
# amenities
amenities <-  grep("^d_.*", names(data), value = TRUE)
# delete from amenities vector first 3 columns
amenities <- amenities[-c(1:4)]
X1  <- c("f_room_type*d_private_balcony", "f_room_type*d_view","f_room_type*f_neighbourhood_cleansed",
"f_property_type*f_neighbourhood_cleansed")
# Additional interactions of factors and dummies
X2  <- c("d_netflix*f_room_type","d_bathtub*f_room_type", "d_outdoor_furniture*f_room_type")
predictors_1 <- c(basic_vars)
predictors_2 <- c(basic_vars, reviews, amenities)
predictors_E <- c(basic_vars, reviews, amenities, X1,X2)
#########################################################################################
# RANDOM FORESTS -------------------------------------------------------
#
#########################################################################################
# do 5-fold CV
train_control <- trainControl(method = "cv",
number = 5,
verboseIter = FALSE)
# set tuning for benchamrk model (2)
tune_grid <- expand.grid(
.mtry = c(8, 10, 12),
.splitrule = "variance",
.min.node.size = c(5, 10, 15)
)
set.seed(1234)
system.time({
rf_model_2 <- train(
formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
data = data_train,
method = "ranger",
trControl = train_control,
tuneGrid = tune_grid,
importance = "impurity"
)
})
rf_model_2
rf_tuning_modelB <- rf_model_2$results %>%
dplyr::select(mtry, min.node.size, RMSE) %>%
dplyr::rename(nodes = min.node.size) %>%
spread(key = mtry, value = RMSE)
kable(x = rf_tuning_modelB, format = "latex", digits = 2, caption = "CV RMSE") %>%
add_header_above(c(" ", "vars" = 3))
result_1 <- matrix(c(
rf_model_1$finalModel$mtry,
rf_model_2$finalModel$mtry,
rf_model_2auto$finalModel$mtry,
rf_model_1$finalModel$min.node.size,
rf_model_2$finalModel$min.node.size,
rf_model_2auto$finalModel$min.node.size
),
nrow=3, ncol=2,
dimnames = list(c("Model A", "Model B","Model B auto"),
c("Min vars","Min nodes"))
)
group.importance <- function(rf.obj, groups) {
var.imp <- as.matrix(sapply(groups, function(g) {
sum(importance(rf.obj)[g], na.rm = TRUE)
}))
colnames(var.imp) <- "MeanDecreaseGini"
return(var.imp)
}
rf_model_2_var_imp <- importance(rf_model_2$finalModel)/1000
rf_model_2_var_imp_df <-
data.frame(varname = names(rf_model_2_var_imp),imp = rf_model_2_var_imp) %>%
mutate(varname = gsub("f_neighbourhood_cleansed", "Borough:", varname) ) %>%
mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
arrange(desc(imp)) %>%
mutate(imp_percentage = imp/sum(imp))
plot(varImp(rf_model_2))
cutoff = 600
rf_model_2_var_imp_plot <- ggplot(rf_model_2_var_imp_df[rf_model_2_var_imp_df$imp>cutoff,],
aes(x=reorder(varname, imp), y=imp_percentage)) +
geom_point(color=color[1], size=1.5) +
geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=1) +
ylab("Importance (Percent)") +
xlab("Variable Name") +
coord_flip() +
scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
theme_bg() +
theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))
rf_model_2_var_imp_plot
# to have a quick look
plot(varImp(rf_model_2))
cutoff = 1000
rf_model_2_var_imp_plot <- ggplot(rf_model_2_var_imp_df[rf_model_2_var_imp_df$imp>cutoff,],
aes(x=reorder(varname, imp), y=imp_percentage)) +
geom_point(color=color[1], size=1.5) +
geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=1) +
ylab("Importance (Percent)") +
xlab("Variable Name") +
coord_flip() +
scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
theme_bg() +
theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))
rf_model_2_var_imp_plot
plot(varImp(rf_model_2))
cutoff = 1500
rf_model_2_var_imp_plot <- ggplot(rf_model_2_var_imp_df[rf_model_2_var_imp_df$imp>cutoff,],
aes(x=reorder(varname, imp), y=imp_percentage)) +
geom_point(color=color[1], size=1.5) +
geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=1) +
ylab("Importance (Percent)") +
xlab("Variable Name") +
coord_flip() +
scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
theme_bg() +
theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))
rf_model_2_var_imp_plot
pdp_n_roomtype <- pdp::partial(rf_model_2, pred.var = "f_room_type", pred.grid = distinct_(data_holdout, "f_room_type"), train = data_train)
pdp_n_roomtype_plot <- pdp_n_roomtype %>%
autoplot( ) +
geom_point(color=color[1], size=2) +
ylab("Predicted price") +
xlab("Room type") +
theme_bg()
pdp_n_roomtype_plot
pdp_n_acc <- pdp::partial(rf_model_2, pred.var = "n_accommodates", pred.grid = distinct_(data_holdout, "n_accommodates"), train = data_train)
pdp_n_acc_plot <- pdp_n_acc %>%
autoplot( ) +
geom_point(color=color[1], size=2) +
geom_line(color=color[1], size=1) +
ylab("Predicted price") +
xlab("Accommodates (persons)") +
scale_x_continuous(limit=c(2,6), breaks=seq(1,6,1))+
theme_bg()
pdp_n_acc_plot
gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 10), # complexity of the tree
n.trees = (4:10)*50, # number of iterations, i.e. trees
shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)
set.seed(1234)
system.time({
gbm_model <- train(formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
data = data_train,
method = "gbm",
trControl = train_control,
verbose = FALSE,
tuneGrid = gbm_grid)
})
gbm_model
# OLS
basic_lev_ols  <- c("n_accommodates","n_beds","n_bedroom_count", "f_room_type")
basic_add_ols <- c("f_bathroom","f_minimum_nights","f_neighbourhood_cleansed")
reviews_ols <- c("f_number_of_reviews","d_host_is_superhost",
"d_host_has_profile_pic","d_host_identity_verified","d_instant_bookable")
poly_lev_ols <- c("n_accommodates2", "n_amenities_count2", "n_beds2")
set.seed(1234)
system.time({
ols_model <- train(
formula(paste0(" price ~",paste(c(basic_lev_ols,basic_add_ols,reviews_ols,poly_lev_ols),collapse = " + "))),
data = data_train,
method = "lm",
trControl = train_control
)
})
ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
"variable" = names(ols_model_coeffs),
"ols_coefficient" = ols_model_coeffs
) %>%
mutate(variable = gsub("`","",variable))
# All three models
final_models <-
list("OLS" = ols_model,
"Random forest" = rf_model_2,
"Random forest (auto tuned)" = rf_model_2auto,
"GBM (basic tuning)"  = gbm_model)
final_models <-
list("OLS" = ols_model,
"Random forest" = rf_model_2,
"GBM (basic tuning)"  = gbm_model)
results <- resamples(final_models) %>% summary()
result_4 <- imap(final_models, ~{
mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
rename("CV RMSE" = ".")
kable(x = result_4, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
result_5 <- map(final_models, ~{
RMSE(predict(.x, newdata = data_holdout), data_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
rename("Holdout RMSE" = ".")
kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
### All models
# OLS
basic_lev_ols  <- c("n_accommodates","n_beds","n_bedroom_count", "f_room_type")
basic_add_ols <- c("f_bathroom","f_minimum_nights","f_neighbourhood_cleansed")
reviews_ols <- c("f_number_of_reviews","d_host_is_superhost",
"d_host_has_profile_pic","d_host_identity_verified","d_instant_bookable")
poly_lev_ols <- c("n_accommodates2", "n_amenities_count2", "n_beds2")
set.seed(1234)
system.time({
ols_model <- train(
formula(paste0(" price ~",paste(c(basic_lev_ols,basic_add_ols,reviews_ols,poly_lev_ols),collapse = " + "))),
data = data_train,
method = "lm",
trControl = train_control
)
})
ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
"variable" = names(ols_model_coeffs),
"ols_coefficient" = ols_model_coeffs
) %>%
mutate(variable = gsub("`","",variable))
# All three models
final_models <-
list("OLS" = ols_model,
"Random forest" = rf_model_2,
"GBM (basic tuning)"  = gbm_model)
results <- resamples(final_models) %>% summary()
result_4 <- imap(final_models, ~{
mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
rename("CV RMSE" = ".")
kable(x = result_4, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
result_5 <- map(final_models, ~{
RMSE(predict(.x, newdata = data2), data2[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
rename("Holdout RMSE" = ".")
kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
final_models <-
list("OLS" = ols_model,
"Random forest" = rf_model_2,
"GBM (basic tuning)"  = gbm_model)
results <- resamples(final_models) %>% summary()
result_4 <- imap(final_models, ~{
mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
rename("CV RMSE" = ".")
kable(x = result_4, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
result_5 <- map(final_models, ~{
RMSE(predict(.x, newdata = data), data[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
rename("Holdout RMSE" = ".")
kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
# All three models
final_models <-
list("OLS" = ols_model,
"Random forest" = rf_model_2,
"GBM (basic tuning)"  = gbm_model)
results <- resamples(final_models) %>% summary()
result_4 <- imap(final_models, ~{
mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
rename("CV RMSE" = ".")
kable(x = result_4, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
result_5 <- map(final_models, ~{
RMSE(predict(.x, newdata = data2), data2[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
rename("Holdout RMSE" = ".")
kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
data_holdout2 <- data2[-train_indices, ]
final_models <-
list("OLS" = ols_model,
"Random forest" = rf_model_2,
"GBM (basic tuning)"  = gbm_model)
results <- resamples(final_models) %>% summary()
result_4 <- imap(final_models, ~{
mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
rename("CV RMSE" = ".")
kable(x = result_4, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
result_5 <- map(final_models, ~{
RMSE(predict(.x, newdata = data_holdout2), data_holdout2[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
rename("Holdout RMSE" = ".")
kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
result_5 <- map(final_models, ~{
RMSE(predict(.x, newdata = data_holdout), data_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
rename("Holdout RMSE" = ".")
kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
result_5 <- map(final_models, ~{
RMSE(predict(.x, newdata = data_holdout2), data_holdout2[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
rename("Holdout RMSE" = ".")
kable(x = result_5, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
##############################
# 1) full varimp plot, above a cutoff
##############################
# to have a quick look
plot(varImp(rf_model_2))
cutoff = 1500
rf_model_2_var_imp_plot <- ggplot(rf_model_2_var_imp_df[rf_model_2_var_imp_df$imp>cutoff,],
aes(x=reorder(varname, imp), y=imp_percentage)) +
geom_point(color=color[1], size=1.5) +
geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=1) +
ylab("Importance (Percent)") +
xlab("Variable Name") +
coord_flip() +
scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
theme_bg() +
theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))
rf_model_2_var_imp_plot
# save plot in the graphs folder
ggsave("graphs/feature_import_plot.png", width = 6, height = 4, units = "in", dpi = 300)
pdp_n_acc <- pdp::partial(rf_model_2, pred.var = "n_accommodates", pred.grid = distinct_(data_holdout, "n_accommodates"), train = data_train)
pdp_n_acc_plot <- pdp_n_acc %>%
autoplot( ) +
geom_point(color=color[1], size=2) +
geom_line(color=color[1], size=1) +
ylab("Predicted price") +
xlab("Accommodates (persons)") +
scale_x_continuous(limit=c(2,6), breaks=seq(1,6,1))+
theme_bg()
pdp_n_acc_plot
pdp_n_roomtype <- pdp::partial(rf_model_2, pred.var = "f_room_type", pred.grid = distinct_(data_holdout, "f_room_type"), train = data_train)
pdp_n_roomtype_plot <- pdp_n_roomtype %>%
autoplot( ) +
geom_point(color=color[1], size=2) +
ylab("Predicted price") +
xlab("Room type") +
theme_bg()
pdp_n_roomtype_plot
library(treeshap)
library(treeshap)
#define one-hot encoding function
dummy <- dummyVars(" ~ .", data=data_holdout, fullRank=T, sep = NULL)
#perform one-hot encoding on data frame
data_holdout_ohe <- data.frame(predict(dummy, newdata=data_holdout))
# replace "." character to " " to match model object names
names(data_holdout_ohe) <- gsub(x = names(data_holdout_ohe),
pattern = "\\.",
replacement = " ")
# unify model for treeshap
rf_model_unified <- ranger.unify(rf_model_2$finalModel, data_holdout_ohe)
plot_contribution(treeshap_res, obs = 12)
dummy <- dummyVars(" ~ .", data=data_holdout, fullRank=T, sep = NULL)
#perform one-hot encoding on data frame
data_holdout_ohe <- data.frame(predict(dummy, newdata=data_holdout))
# replace "." character to " " to match model object names
names(data_holdout_ohe) <- gsub(x = names(data_holdout_ohe),
pattern = "\\.",
replacement = " ")
# unify model for treeshap
rf_model_unified <- ranger.unify(rf_model_2$finalModel, data_holdout_ohe)
